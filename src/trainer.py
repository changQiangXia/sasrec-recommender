"""
Trainer
=======
ä¿®å¤åçš„è®­ç»ƒå™¨

å…³é”®ä¿®å¤ï¼š
1. æ­£ç¡®çš„è¯„ä¼°é€»è¾‘ï¼šä½¿ç”¨å›ºå®šè´Ÿæ ·æœ¬æ± è¿›è¡Œé‡‡æ ·è¯„ä¼°ï¼ˆ1æ­£ + Nè´Ÿï¼‰
2. å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šWarmup + Cosine Annealing
3. ä¿®å¤æ ‡ç­¾ç¡¬ç¼–ç é—®é¢˜
4. å¢åŠ è®­ç»ƒç¨³å®šæ€§ç›‘æ§
"""

import os
import math
import warnings
import torch
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import numpy as np

# è¿‡æ»¤æ‰ä¸å½±å“åŠŸèƒ½çš„ PyTorch è­¦å‘Š
warnings.filterwarnings('ignore', message='Support for mismatched src_key_padding_mask and mask is deprecated')
warnings.filterwarnings('ignore', message='enable_nested_tensor is True, but self.use_nested_tensor is False')


class WarmupCosineScheduler:
    """
    Warmup + Cosine Annealing å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    é€‚ç”¨äºå¤§ Batch Size è®­ç»ƒï¼š
    - Warmup é˜¶æ®µï¼šçº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼Œç¨³å®šè®­ç»ƒåˆæœŸ
    - Cosine é˜¶æ®µï¼šä½™å¼¦é€€ç«ï¼Œç²¾ç»†è°ƒæ•´
    """
    
    def __init__(self, optimizer, warmup_steps, total_steps, base_lr, min_lr=1e-6):
        """
        Args:
            optimizer: PyTorch ä¼˜åŒ–å™¨
            warmup_steps: warmup æ­¥æ•°
            total_steps: æ€»è®­ç»ƒæ­¥æ•°
            base_lr: åˆå§‹å­¦ä¹ ç‡
            min_lr: æœ€å°å­¦ä¹ ç‡
        """
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.current_step = 0
    
    def step(self):
        """æ‰§è¡Œä¸€æ­¥è°ƒåº¦"""
        self.current_step += 1
        lr = self.get_lr()
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        return lr
    
    def get_lr(self):
        """è®¡ç®—å½“å‰å­¦ä¹ ç‡"""
        if self.current_step < self.warmup_steps:
            # Warmup é˜¶æ®µï¼šçº¿æ€§å¢åŠ 
            return self.base_lr * (self.current_step / self.warmup_steps)
        else:
            # Cosine Annealing é˜¶æ®µ
            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)
            progress = min(1.0, progress)  # é˜²æ­¢è¶…å‡º
            cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))
            return self.min_lr + (self.base_lr - self.min_lr) * cosine_decay


class Trainer:
    """
    SASRec è®­ç»ƒå™¨ - ä¿®å¤ç‰ˆ
    """
    
    def __init__(self, model, config, train_loader, val_loader, test_loader):
        self.model = model.to(config.device)
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        
        # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
        self.total_steps = config.epochs * len(train_loader)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.lr,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.98)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = WarmupCosineScheduler(
            self.optimizer,
            warmup_steps=config.warmup_steps,
            total_steps=self.total_steps,
            base_lr=config.lr,
            min_lr=config.min_lr
        )
        
        # æ··åˆç²¾åº¦
        self.use_amp = config.use_amp
        self.scaler = GradScaler() if config.use_amp else None
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(config.checkpoint_dir, exist_ok=True)
        
        # æœ€ä½³æŒ‡æ ‡è¿½è¸ª
        self.best_metric = 0.0
        self.best_epoch = 0
        self.metrics_history = []
    
    def train_epoch(self, epoch):
        """
        è®­ç»ƒä¸€ä¸ª epoch
        
        Returns:
            avg_loss: å¹³å‡æŸå¤±
        """
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")
        
        for batch_idx, (seqs, pos_items, neg_items) in enumerate(pbar):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            seqs = seqs.to(self.config.device, non_blocking=True)
            pos_items = pos_items.to(self.config.device, non_blocking=True)
            neg_items = neg_items.to(self.config.device, non_blocking=True)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ ·æœ¬
            if (pos_items > 0).sum() == 0:
                continue
            
            self.optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.use_amp:
                with autocast():
                    loss = self.model.compute_loss(seqs, pos_items, neg_items)
                
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss = self.model.compute_loss(seqs, pos_items, neg_items)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            current_lr = self.scheduler.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{total_loss/num_batches:.4f}",
                'lr': f"{current_lr:.2e}"
            })
        
        return total_loss / max(num_batches, 1)
    
    @torch.no_grad()
    def evaluate(self, loader, split="val"):
        """
        è¯„ä¼°æ¨¡å‹ - ä½¿ç”¨å›ºå®šè´Ÿæ ·æœ¬æ± è¿›è¡Œé‡‡æ ·è¯„ä¼°
        
        è¯„ä¼°é€»è¾‘ï¼š
        1. å¯¹äºæ¯ä¸ªç”¨æˆ·ï¼Œç»™å®šè¾“å…¥åºåˆ—
        2. ç›®æ ‡æ˜¯ä» {1æ­£æ ·æœ¬ + Nè´Ÿæ ·æœ¬} ä¸­é€‰å‡ºæ­£æ ·æœ¬
        3. è®¡ç®— Hit Ratio@K å’Œ NDCG@K
        
        Args:
            loader: æ•°æ®åŠ è½½å™¨ï¼ˆSequenceEvalDatasetï¼‰
            split: æ•°æ®é›†åç§°ï¼ˆval/testï¼‰
        
        Returns:
            metrics: åŒ…å« HR@K, NDCG@K, MRR çš„å­—å…¸
        """
        self.model.eval()
        
        all_ranks = []  # æ”¶é›†æ‰€æœ‰æ­£æ ·æœ¬çš„æ’å
        all_scores = []  # æ”¶é›†åˆ†æ•°ç”¨äºè°ƒè¯•
        
        for batch in tqdm(loader, desc=f"Eval {split}"):
            seqs, targets, neg_pools = batch
            
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            seqs = seqs.to(self.config.device, non_blocking=True)
            targets = targets.to(self.config.device, non_blocking=True)
            neg_pools = neg_pools.to(self.config.device, non_blocking=True)
            
            batch_size = seqs.size(0)
            
            # è¿‡æ»¤æ— æ•ˆæ ·æœ¬ï¼ˆç›®æ ‡ä¸º0è¡¨ç¤ºåºåˆ—å¤ªçŸ­ï¼‰
            valid_mask = targets > 0
            if valid_mask.sum() == 0:
                continue
            
            valid_seqs = seqs[valid_mask]
            valid_targets = targets[valid_mask]
            valid_neg_pools = neg_pools[valid_mask]
            valid_batch_size = valid_seqs.size(0)
            
            # æ„å»ºå€™é€‰é›†ï¼š1ä¸ªæ­£æ ·æœ¬ + Nä¸ªè´Ÿæ ·æœ¬
            # candidates[i] = [target_i, neg_pool_i[0], neg_pool_i[1], ...]
            candidates = torch.cat([
                valid_targets.unsqueeze(1),  # (B, 1)
                valid_neg_pools              # (B, num_neg_samples)
            ], dim=1)  # (B, 1 + num_neg_samples)
            
            # æ‰¹é‡é¢„æµ‹
            logits = self.model.predict(valid_seqs, candidates)  # (B, 1 + num_neg)
            
            # è®¡ç®—æ’åï¼ˆåˆ†æ•°é™åºæ’åˆ—ï¼Œæ­£æ ·æœ¬åœ¨ä½ç½®0ï¼‰
            # æ³¨æ„ï¼šæ­£æ ·æœ¬æ€»æ˜¯åœ¨ candidates çš„ç¬¬ 0 åˆ—
            rankings = torch.argsort(logits, descending=True, dim=1)  # (B, num_candidates)
            
            # æ‰¾åˆ°æ­£æ ·æœ¬ï¼ˆåˆ—0ï¼‰çš„æ’å
            # rankings[i, j] è¡¨ç¤ºç¬¬ i ä¸ªæ ·æœ¬ä¸­åŸç¬¬ j åˆ—çš„æ–°ä½ç½®
            # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°åˆ—0çš„æ–°ä½ç½®
            for i in range(valid_batch_size):
                # æ‰¾åˆ° 0 åœ¨ rankings[i] ä¸­çš„ä½ç½®
                rank = (rankings[i] == 0).nonzero(as_tuple=True)[0].item() + 1  # ä»1å¼€å§‹è®¡æ•°
                all_ranks.append(rank)
        
        if len(all_ranks) == 0:
            return {f'HR@{k}': 0.0 for k in self.config.top_k_list} | {f'NDCG@{k}': 0.0 for k in self.config.top_k_list} | {'MRR': 0.0}
        
        # è½¬æ¢ä¸º numpy æ•°ç»„
        all_ranks = np.array(all_ranks)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        for k in self.config.top_k_list:
            # HR@K: æ­£æ ·æœ¬æ’å <= K çš„æ¯”ä¾‹
            metrics[f'HR@{k}'] = np.mean(all_ranks <= k)
            
            # NDCG@K: è€ƒè™‘æ’åçš„æŠ˜æ‰£ç´¯ç§¯å¢ç›Š
            # DCG = 1 / log2(rank + 1) å¦‚æœ rank <= Kï¼Œå¦åˆ™ 0
            # å› ä¸ºåªæœ‰ä¸€ä¸ªæ­£æ ·æœ¬ï¼ŒIDCG = 1ï¼ˆç†æƒ³æƒ…å†µä¸‹æ’ç¬¬1ï¼‰
            dcg = np.where(all_ranks <= k, 1.0 / np.log2(all_ranks + 1), 0.0)
            metrics[f'NDCG@{k}'] = np.mean(dcg)
        
        # MRR: å¹³å‡å€’æ•°æ’å
        metrics['MRR'] = np.mean(1.0 / all_ranks)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        metrics['mean_rank'] = np.mean(all_ranks)
        metrics['median_rank'] = np.median(all_ranks)
        
        return metrics
    
    def save_checkpoint(self, epoch, metric):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        checkpoint_path = os.path.join(self.config.checkpoint_dir, 'best.pt')
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': {
                'current_step': self.scheduler.current_step,
                'warmup_steps': self.scheduler.warmup_steps,
                'total_steps': self.scheduler.total_steps,
                'base_lr': self.scheduler.base_lr,
                'min_lr': self.scheduler.min_lr,
            },
            'metric': metric,
            'config': self.config
        }, checkpoint_path)
        print(f"ğŸ’¾ Best model saved (NDCG@10={metric:.4f})")
    
    def load_checkpoint(self, path):
        """åŠ è½½æ¨¡å‹"""
        ckpt = torch.load(path, map_location=self.config.device)
        self.model.load_state_dict(ckpt['model_state_dict'])
        
        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
        if 'optimizer_state_dict' in ckpt:
            self.optimizer.load_state_dict(ckpt['optimizer_state_dict'])
        
        # æ¢å¤è°ƒåº¦å™¨çŠ¶æ€
        if 'scheduler_state_dict' in ckpt:
            sched_state = ckpt['scheduler_state_dict']
            self.scheduler.current_step = sched_state.get('current_step', 0)
            self.scheduler.warmup_steps = sched_state.get('warmup_steps', self.config.warmup_steps)
            self.scheduler.total_steps = sched_state.get('total_steps', self.total_steps)
            self.scheduler.base_lr = sched_state.get('base_lr', self.config.lr)
            self.scheduler.min_lr = sched_state.get('min_lr', self.config.min_lr)
        
        print(f"âœ… Loaded checkpoint from epoch {ckpt.get('epoch', 'unknown')}")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ Training on {self.config.device}, AMP={self.use_amp}")
        print(f"   Total steps: {self.total_steps}, Warmup: {self.config.warmup_steps}")
        print(f"   Eval neg samples: {self.config.eval_neg_samples}")
        
        for epoch in range(1, self.config.epochs + 1):
            # è®­ç»ƒ
            avg_loss = self.train_epoch(epoch)
            print(f"Epoch {epoch}: Train Loss = {avg_loss:.4f}, LR = {self.scheduler.get_lr():.2e}")
            
            # è¯„ä¼°
            if epoch % 5 == 0 or epoch == 1:
                val_metrics = self.evaluate(self.val_loader, "val")
                
                # æ‰“å°è¯¦ç»†æŒ‡æ ‡
                print(f"  Val:  HR@10={val_metrics['HR@10']:.4f}, NDCG@10={val_metrics['NDCG@10']:.4f}, MRR={val_metrics['MRR']:.4f}")
                print(f"        Mean Rank={val_metrics['mean_rank']:.1f}, Median Rank={val_metrics['median_rank']:.1f}")
                
                # ä¿å­˜å†å²
                self.metrics_history.append({
                    'epoch': epoch,
                    'loss': avg_loss,
                    **val_metrics
                })
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_metrics['NDCG@10'] > self.best_metric:
                    self.best_metric = val_metrics['NDCG@10']
                    self.best_epoch = epoch
                    self.save_checkpoint(epoch, self.best_metric)
                
                # æ—©åœæ£€æŸ¥
                if epoch - self.best_epoch >= self.config.early_stop_patience:
                    print(f"â¹ï¸ Early stopping at epoch {epoch} (no improvement for {self.config.early_stop_patience} epochs)")
                    break
        
        # è®­ç»ƒç»“æŸï¼Œè¾“å‡ºæœ€ç»ˆç»“æœ
        print(f"\n{'='*60}")
        print(f"ğŸ Training Complete")
        print(f"   Best epoch: {self.best_epoch}")
        print(f"   Best Val NDCG@10: {self.best_metric:.4f}")
        print(f"{'='*60}")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
        best_path = os.path.join(self.config.checkpoint_dir, 'best.pt')
        if os.path.exists(best_path):
            self.load_checkpoint(best_path)
            test_metrics = self.evaluate(self.test_loader, "test")
            
            print(f"\nğŸ“ Test Results:")
            print(f"   HR@10={test_metrics['HR@10']:.4f}, NDCG@10={test_metrics['NDCG@10']:.4f}, MRR={test_metrics['MRR']:.4f}")
            print(f"   Mean Rank={test_metrics['mean_rank']:.1f}, Median Rank={test_metrics['median_rank']:.1f}")
            
            return test_metrics
        else:
            print("âš ï¸ No checkpoint found, skipping test evaluation")
            return {}

#!/usr/bin/env python3
"""
SASRec Recommender System - Main Entry
======================================
åŸºäº Transformer çš„åºåˆ—æ¨èç³»ç»Ÿä¸»å…¥å£ - ä¿®å¤ç‰ˆ

å…³é”®ä¿®å¤ï¼š
1. æ­£ç¡®å¤„ç† torch.device
2. æ›´å¥½çš„é”™è¯¯å¤„ç†
3. æ¸…æ™°çš„æ—¥å¿—è¾“å‡º
"""

import os
import sys
import argparse
import torch

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.config import get_config, Config
from src.data_loader import get_data_loaders
from src.model import SASRec
from src.trainer import Trainer
from src.utils import set_seed, count_parameters


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='SASRec Training and Evaluation',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # æ•°æ®é›†
    parser.add_argument('--dataset', type=str, default='movielens',
                       choices=['movielens', 'taobao'],
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--data_dir', type=str, default=None,
                       help='æ•°æ®ç›®å½•ï¼ˆé»˜è®¤: ./data/{dataset}ï¼‰')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--hidden_units', type=int, default=128,
                       help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--num_blocks', type=int, default=2,
                       help='Transformer å±‚æ•°')
    parser.add_argument('--num_heads', type=int, default=4,
                       help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--dropout', type=float, default=0.2,
                       help='Dropout ç‡')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=2048,
                       help='è®­ç»ƒ batch size')
    parser.add_argument('--epochs', type=int, default=200,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=0.001,
                       help='åˆå§‹å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--warmup_steps', type=int, default=2000,
                       help='Warmup æ­¥æ•°')
    
    # è´Ÿé‡‡æ ·
    parser.add_argument('--neg_strategy', type=str, default='mixed',
                       choices=['random', 'popular', 'mixed'],
                       help='è´Ÿé‡‡æ ·ç­–ç•¥')
    parser.add_argument('--popular_alpha', type=float, default=0.75,
                       help='æ··åˆé‡‡æ ·ä¸­çƒ­é—¨ç‰©å“æ¯”ä¾‹')
    parser.add_argument('--eval_neg_samples', type=int, default=100,
                       help='è¯„ä¼°æ—¶è´Ÿæ ·æœ¬æ•°')
    
    # æ··åˆç²¾åº¦
    parser.add_argument('--no_amp', action='store_true',
                       help='ç¦ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦')
    
    # å…¶ä»–
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--num_workers', type=int, default=0,
                       help='DataLoader å·¥ä½œè¿›ç¨‹æ•°')
    
    # æ¨¡å¼
    parser.add_argument('--mode', type=str, default='train',
                       choices=['train', 'eval'],
                       help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--checkpoint', type=str, default=None,
                       help='è¯„ä¼°æ—¶åŠ è½½çš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--resume', type=str, default=None,
                       help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # æ£€æµ‹è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\n{'='*60}")
    print(f"ğŸ”¥ Device: {device}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   CUDA: {torch.version.cuda}")
        print(f"   PyTorch: {torch.__version__}")
    print(f"{'='*60}")
    
    # è·å–é…ç½®
    config = get_config(args.dataset)
    
    # ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    config.device = device
    config.data_dir = args.data_dir if args.data_dir else f"./data/{args.dataset}"
    config.hidden_units = args.hidden_units
    config.num_blocks = args.num_blocks
    config.num_heads = args.num_heads
    config.dropout = args.dropout
    config.batch_size = args.batch_size
    config.epochs = args.epochs
    config.lr = args.lr
    config.weight_decay = args.weight_decay
    config.warmup_steps = args.warmup_steps
    config.neg_sampling_strategy = args.neg_strategy
    config.popular_items_alpha = args.popular_alpha
    config.eval_neg_samples = args.eval_neg_samples
    config.use_amp = (not args.no_amp) and torch.cuda.is_available()
    config.seed = args.seed
    config.num_workers = args.num_workers
    
    # æ‰“å°é…ç½®
    print(f"\nğŸ“‹ Configuration:")
    print(config)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“‚ Loading data...")
    try:
        train_loader, val_loader, test_loader, stats = get_data_loaders(config)
    except FileNotFoundError as e:
        print(f"\nâŒ Error: {e}")
        print(f"\nè¯·ä¸‹è½½ MovieLens 25M æ•°æ®é›†:")
        print(f"  wget https://files.grouplens.org/datasets/movielens/ml-25m.zip")
        print(f"  unzip ml-25m.zip -d ./data/movielens/")
        return 1
    except Exception as e:
        print(f"\nâŒ Error loading data: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    num_items = stats['num_items']
    print(f"\nğŸ“Š Data Statistics:")
    print(f"   Users: {stats['num_users']:,}")
    print(f"   Items: {num_items:,}")
    print(f"   Avg Seq Len: {stats['avg_seq_len']:.1f}")
    print(f"   Train batches: {len(train_loader)}")
    print(f"   Val batches: {len(val_loader)}")
    print(f"   Test batches: {len(test_loader)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸  Building model...")
    model = SASRec(num_items=num_items, config=config)
    num_params = count_parameters(model)
    print(f"   Parameters: {num_params:,} ({num_params/1e6:.2f}M)")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(model, config, train_loader, val_loader, test_loader)
    
    # æ¢å¤æ£€æŸ¥ç‚¹
    if args.resume:
        if not os.path.exists(args.resume):
            print(f"\nâŒ Checkpoint not found: {args.resume}")
            return 1
        print(f"\nğŸ“¥ Resuming from {args.resume}")
        trainer.load_checkpoint(args.resume)
    
    # è®­ç»ƒæˆ–è¯„ä¼°
    if args.mode == 'train':
        print("\nğŸƒ Starting training...")
        try:
            test_metrics = trainer.train()
            print("\nâœ… Training completed successfully!")
            return 0
        except KeyboardInterrupt:
            print("\n\nâš ï¸ Training interrupted by user")
            # ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹
            interrupt_path = os.path.join(config.checkpoint_dir, 'interrupted.pt')
            trainer.save_checkpoint(trainer.scheduler.current_step // len(train_loader), 0)
            print(f"Checkpoint saved to {interrupt_path}")
            return 0
        except Exception as e:
            print(f"\nâŒ Training failed: {e}")
            import traceback
            traceback.print_exc()
            return 1
    
    elif args.mode == 'eval':
        if args.checkpoint is None:
            print("âŒ Error: --checkpoint required for eval mode")
            return 1
        if not os.path.exists(args.checkpoint):
            print(f"âŒ Error: Checkpoint not found: {args.checkpoint}")
            return 1
        
        print(f"\nğŸ“Š Evaluating {args.checkpoint}")
        trainer.load_checkpoint(args.checkpoint)
        
        val_metrics = trainer.evaluate(val_loader, "val")
        test_metrics = trainer.evaluate(test_loader, "test")
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        print(f"\nğŸ“ˆ Evaluation Results:")
        print(f"{'='*60}")
        print(f"Val Results:")
        print(f"  HR@10:  {val_metrics['HR@10']:.4f}")
        print(f"  NDCG@10: {val_metrics['NDCG@10']:.4f}")
        print(f"  MRR:    {val_metrics['MRR']:.4f}")
        print(f"\nTest Results:")
        print(f"  HR@10:  {test_metrics['HR@10']:.4f}")
        print(f"  NDCG@10: {test_metrics['NDCG@10']:.4f}")
        print(f"  MRR:    {test_metrics['MRR']:.4f}")
        print(f"{'='*60}")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        results_dir = "./results"
        os.makedirs(results_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_name = os.path.basename(args.checkpoint).replace('.pt', '')
        result_file = os.path.join(results_dir, f"eval_{checkpoint_name}_{timestamp}.txt")
        
        # å†™å…¥ç»“æœ
        with open(result_file, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("SASRec Evaluation Results\n")
            f.write("="*60 + "\n\n")
            
            f.write(f"Evaluation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Checkpoint: {args.checkpoint}\n")
            f.write(f"Dataset: {config.dataset}\n")
            f.write(f"Num Items: {num_items:,}\n")
            f.write(f"Num Users: {stats['num_users']:,}\n\n")
            
            f.write("-"*60 + "\n")
            f.write("Validation Results:\n")
            f.write("-"*60 + "\n")
            f.write(f"  HR@10:   {val_metrics['HR@10']:.4f}\n")
            f.write(f"  NDCG@10: {val_metrics['NDCG@10']:.4f}\n")
            f.write(f"  MRR:     {val_metrics['MRR']:.4f}\n")
            f.write(f"  Mean Rank: {val_metrics['mean_rank']:.1f}\n")
            f.write(f"  Median Rank: {val_metrics['median_rank']:.1f}\n\n")
            
            f.write("-"*60 + "\n")
            f.write("Test Results:\n")
            f.write("-"*60 + "\n")
            f.write(f"  HR@10:   {test_metrics['HR@10']:.4f}\n")
            f.write(f"  NDCG@10: {test_metrics['NDCG@10']:.4f}\n")
            f.write(f"  MRR:     {test_metrics['MRR']:.4f}\n")
            f.write(f"  Mean Rank: {test_metrics['mean_rank']:.1f}\n")
            f.write(f"  Median Rank: {test_metrics['median_rank']:.1f}\n\n")
            
            # å…¶ä»–æŒ‡æ ‡
            for k in [5, 10, 20]:
                if f'HR@{k}' in val_metrics:
                    f.write(f"  HR@{k}:    {val_metrics[f'HR@{k}']:.4f} (Val)  {test_metrics[f'HR@{k}']:.4f} (Test)\n")
                if f'NDCG@{k}' in val_metrics:
                    f.write(f"  NDCG@{k}:  {val_metrics[f'NDCG@{k}']:.4f} (Val)  {test_metrics[f'NDCG@{k}']:.4f} (Test)\n")
            
            f.write("\n" + "="*60 + "\n")
        
        print(f"\nğŸ’¾ Results saved to: {result_file}")
        
        return 0


if __name__ == '__main__':
    sys.exit(main())
